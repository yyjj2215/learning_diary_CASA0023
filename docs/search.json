[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning Diary for CASA0023",
    "section": "",
    "text": "Welcome\nHello!\nWelcome to the learning diary. This is a book created for learning CASA0023 Remotely Sensing Cities and Environments.\nI would like to thank Dr Andrew MacLachlan for helping with this book."
  },
  {
    "objectID": "week1.html#summary",
    "href": "week1.html#summary",
    "title": "1  Week1 Getting started with remote sensing",
    "section": "1.1 Summary",
    "text": "1.1 Summary\n\n1.1.1 what is remote sensing?\nRemote sensing is the process of detecting and monitoring the physical characteristics of an area by measuring its reflected and emitted radiation at a distance (typically from satellite or aircraft). It has two types, one is active remote sensing,the other one is passive remote sensing. The difference between them is active remote sensing has a energy source.\n\n\n\n\n\nDifferences between passive and active remote sensing. Source: Fernanda Lopez Ornelas\n\n\n\n\n\n\n1.1.2 Electromagentic waves\npropagate through space and carry momentum and electromagnetic radiant energy.\n\n\n\n\n\nElectromagentice waves. Source: Encyclopaedia Britannica\n\n\n\n\n\n\n1.1.3 Interactions with atmosphere and earth’s surface\n\n\n\n\n\n\n\n\n\nAtmosphere\nEarth’s suface\n\n\n\n\nInteraction\nscattered by particles\n\nabsorbed by the surface\ntransmitted through the surface\n\n\n\n\nScattering has three types:\na) Raleigh and Mie\n\n\n\n\n\nSource: physicsOpenLab\n\n\n\n\nb) non-selective\n\n\n\n\n\nSource: Natural Resources Canada\n\n\n\n\nRaleigh scattering(Why sky is blue? and sunset is red?)\nBlue light is scattered more than red light so the sky appears blue. The light has to travel further through the Earth’s atmosphere in sunset. The blue light is scattered away, but the red light isn’t scattered very much – so the sky appears red.\n\n\n1.1.4 Four resolutions of remote sensing data\n\n\n\n\n\n\n\n\nResolution Types\nDescription\nExamples\n\n\n\n\nspatial resolution\nthe size of the raster grid per pixel\n20cm or 30m\n\n\nspectral resolution\nthe number of bands\nBand 2 - blue (0.45-0.51 wavelength)\n\n\ntemporal resolution\nthe time it revisits\ndaily or every 7 days\n\n\nRadiometric resolution\nthe range of possible values\n8 bit, 12 bit, or 16 bit\n\n\n\n\n\n1.1.5 Raster data acquisition\nThere are many sources that provide raster data, for example, raster data can be produced by aerial photography and satellites. The common satellites data can be downloaded in two websites:\nSentinel data\nThe data is provided by the Copernicus Space Component Data Access (CSCDA), which is operating by the European Space Agency. The data has high level information, free, full, and open for all international users.\nData download link: https://scihub.copernicus.eu/dhus/#/home\nLandsat data All Landsat data are provided by the U.S. Geological Survey, and it’s also free. It has the longest free temporal data, which is useful for doing raster data analysis in temporal aspects.\nData download link: https://earthexplorer.usgs.gov/\n\n\n1.1.6 Analysis tool\nSNAP (Sentinels Application Platform) is a raster data analysis tool, and it is designed for Sentinels data. It also can manipulate Landsat data, so it has strong analysis functions. For example, resampling data, reporjecting, masking, and so on.\n\n\n\n\n\nSNAP layout Source: Yanbo 2023"
  },
  {
    "objectID": "week1.html#application",
    "href": "week1.html#application",
    "title": "1  Week1 Getting started with remote sensing",
    "section": "1.2 Application",
    "text": "1.2 Application\nColor composites are an essential concept in remote sensing. True color composites sometimes refer to the color we recognize in life. For example, the tree canopy is green, and the strawberry is red. The false color composites are to display the real color in other colors. For example, the tree canopy can be displayed as red, and the strawberry can be displayed as green. The false color composites are widely applied in many remote sensing researches.\nOne research was conducted in Afghanistan by U.S. Agency for International Development and the U.S. Trade and Development Agency, and it assessed the natural resources of Afghanistan. The following picture is a true color composite. It’s difficult to recognize some of the characteristics in this area., but the False color composite better displays individual strata, allows regional correlations of strata, and better depicts structure, such as anticlines, synclines, and faults(Philip A. (2007)).\n\n\n\n\n\nSource: Davis, P. A. 2007\n\n\n\n\n\n\n\n\n\nSource: Davis, P. A. 2007\n\n\n\n\nThe color gun can produce a variety of false color composites. And the second research compares the different combinations of false colors in Pail/Padhrar Area in Punjab Province, Pakistan (Bajwa, Ahsan, and Ahmad (2020)) The table shows the same geological formation can be displayed in various colors in different false color composites. This would be applied to many remote sensing data if we try to identify some specific features.\n\n\n\n\n\nSource: Bajwa et al. 2020"
  },
  {
    "objectID": "week1.html#reflection",
    "href": "week1.html#reflection",
    "title": "1  Week1 Getting started with remote sensing",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\nRaster data acquisition\nThe two main raster data acquisition ways are useful for future research, and I can find all raster data all over the world from Sentinel and Landsat. This actually inspires me that I can applied raster data in my final work of GEOG0114 course last term. My original topic is to explore how the thermal inequality affect people’s health in Chicago, but I don’t know where to download raster data and how to manipulate it. Then, I just gave up this topic and selected a new topic, which was doing criminology research. However, I think I’m currently able to analysis raster data in my study by using SNAP, QGIS, and R.\n\n\n\n\n\nResample and mask for Sentinel and Landsat data of Cape Town in south Africa Source: Yanbo\n\n\n\n\nSpectral signature\nI think this content will be useful for me to better understand the applications of remote sensing data. Spectral signature is the variations of reflectance and absorption for different materials in various wavelengths.\n\n\n\n\n\nTypical spectral signatures of specic land cover type Source: Anna 2017\n\n\n\n\nThe above figure provides a very good example for explanation. I can easily identify different material based on their characteristics of reflectance. For example, when the wavelengths greater than 700nm, the reflectance of green vegetation increases from 10% to 50%, this is also called red edge. This is really essential for identify green vegetation in real application and distinguish different vegetation type because the amount of Chlorophyll are various in different species. In addition, I also can use spectral signature to identify different type of materials, like explore where is the urban area, agriculture, bare land, and so on.\nColor composites\nThis concept is very interesting because I think many people have cognition that tree leaves are green, and flower is colorful. Also, in our phone, they are also displayed as the same color that we recognize in life. If they are displayed into different colors, it comes very interesting things. For example, I can use false color composites to create image has purple or pink leaves if I want. In addition, false composites are essential for remote sensing data because they can show more characteristics in the image compared to true color composites. This has been discussed in the application part, and I think I also can apply false color composites in my future study.\n\n\n\n\nBajwa, Rizwan Saqib, Naveed Ahsan, and Sajid Rashid Ahmad. 2020. “A Review of Landsat False Color Composite Images for Lithological Mapping of Pre-Cambrian to Recent Rocks: A Case Study of Pail/Padhrar Area in Punjab Province, Pakistan.” Journal of the Indian Society of Remote Sensing 48 (5): 721–28. https://doi.org/10.1007/s12524-019-01090-7.\n\n\nPhilip A., Davis. 2007. “Landsat ETM+ False-Color Image Mosaics of Afghanistan.”"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created for learning CASA0023 Remotely Sensing Cities and Environments, and it was written as a learning diary.\nI would like to thank Dr Andrew MacLachlan for helping with this book."
  },
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "2  Week2 Portfolio",
    "section": "",
    "text": "This is my week2 presentation. Please visit link: https://yyjj2215.github.io/week2_Xaringan/."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "10  Summary",
    "section": "",
    "text": "In summary, this book"
  },
  {
    "objectID": "week1.html#what-is-remote-sensing",
    "href": "week1.html#what-is-remote-sensing",
    "title": "2  Week1 Getting started with remote sensing",
    "section": "2.2 what is remote sensing?",
    "text": "2.2 what is remote sensing?\nRemote sensing is the process of detecting and monitoring the physical characteristics of an area by measuring its reflected and emitted radiation at a distance (typically from satellite or aircraft). It has two types, one is active remote sensing,the other one is passive remote sensing. The difference between them is active remote sensing has a energy source.\n\n\n\n[Differences between passive and active remote sensing](https://www.researchgate.net/publication/339726853_The_Mexican_Water_Forest_benefits_of_using_remote_sensing_techniques_to_assess_changes_in_land_use_and_land_cover)"
  },
  {
    "objectID": "week1.html#electromagentic-waves",
    "href": "week1.html#electromagentic-waves",
    "title": "2  Week1 Getting started with remote sensing",
    "section": "2.3 Electromagentic waves",
    "text": "2.3 Electromagentic waves\npropagate through space and carry momentum and electromagnetic radiant energy.\n\n\n\n[Electromagentice waves](https://www.britannica.com/science/electromagnetic-spectrum)"
  },
  {
    "objectID": "week1.html#interactions-with-atmosphere-and-earths-surface",
    "href": "week1.html#interactions-with-atmosphere-and-earths-surface",
    "title": "2  Week1 Getting started with remote sensing",
    "section": "2.4 Interactions with atmosphere and earth’s surface",
    "text": "2.4 Interactions with atmosphere and earth’s surface\n\n\n\n\n\n\n\n\n\nAtmosphere\nEarth’s suface\n\n\n\n\nInteraction\nscattered by particles\n\nabsorbed by the surface\ntransmitted through the surface\n\n\n\n\nScattering has three types:\n\n\n\n\n\nRaleigh scattering(Why sky is blue? and sunset is red?)\nBlue light is scattered more than red light so the sky appears blue. The light has to travel further through the Earth’s atmosphere in sunset. The blue light is scattered away, but the red light isn’t scattered very much – so the sky appears red."
  },
  {
    "objectID": "week1.html#four-resolutions-of-remote-sensing-data",
    "href": "week1.html#four-resolutions-of-remote-sensing-data",
    "title": "2  Week1 Getting started with remote sensing",
    "section": "2.5 Four resolutions of remote sensing data",
    "text": "2.5 Four resolutions of remote sensing data\n\n\n\n\n\n\n\n\nResolution Types\nDescription\nExamples\n\n\n\n\nspatial resolution\nthe size of the raster grid per pixel\n20cm or 30m\n\n\nspectral resolution\nthe number of bands\nBand 2 - blue (0.45-0.51 wavelength)\n\n\ntemporal resolution\nthe time it revisits\ndaily or every 7 days\n\n\nRadiometric resolution\nthe range of possible values\n8 bit, 12 bit, or 16 bit"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bajwa, Rizwan Saqib, Naveed Ahsan, and Sajid Rashid Ahmad. 2020.\n“A Review of Landsat False Color Composite Images for Lithological\nMapping of Pre-Cambrian to Recent Rocks: A Case Study of Pail/Padhrar\nArea in Punjab Province, Pakistan.” Journal of the Indian\nSociety of Remote Sensing 48 (5): 721–28. https://doi.org/10.1007/s12524-019-01090-7.\n\n\nLocke, Dexter H., J. Morgan Grove, Jacqueline W. T. Lu, Austin Troy,\nJarlath P. M. O’Niel-Dunne, and Brian D. Beck. 2010. “Prioritizing\nPreferable Locations for Increasing Urban Tree Canopy in New York\nCity.” Cities and the Environment 3 (1): 1–18. https://doi.org/10.15365/cate.3142010.\n\n\nMoravec, David, Jan Komárek, Serafín López-Cuervo Medina, and Iñigo\nMolina. 2021. “Effect of Atmospheric Corrections on NDVI:\nIntercomparability of Landsat 8, Sentinel-2, and UAV Sensors.”\nRemote Sensing 13 (18): 3550. https://doi.org/10.3390/rs13183550.\n\n\nPhilip A., Davis. 2007. “Landsat ETM+ False-Color Image Mosaics of\nAfghanistan.”\n\n\nPoncet, Aurelie M., Thorsten Knappenberger, Christian Brodbeck, Michael\nFogle, Joey N. Shaw, and Brenda V. Ortiz. 2019a. “Multispectral\nUAS Data Accuracy for Different Radiometric Calibration Methods.”\nRemote Sensing 11 (16): 1917. https://doi.org/10.3390/rs11161917.\n\n\n———. 2019b. “Multispectral UAS Data Accuracy for Different\nRadiometric Calibration Methods.” Remote Sensing 11\n(16): 1917. https://doi.org/10.3390/rs11161917.\n\n\nWu, Chunxia, Qingfu Xiao, and E. Gregory McPherson. 2008. “A\nMethod for Locating Potential Tree-Planting Sites in Urban Areas: A Case\nStudy of Los Angeles, USA.” Urban Forestry & Urban\nGreening 7 (2): 65–76. https://doi.org/10.1016/j.ufug.2008.01.002."
  },
  {
    "objectID": "week3.html#summary",
    "href": "week3.html#summary",
    "title": "3  Week3 Remote sensing data",
    "section": "3.1 Summary",
    "text": "3.1 Summary\n\n3.1.1 How Landsat obtain image data by using scanners?\nThe remote sensing data are collected by multispectral scanners, and they can detect different electromagnetic bands of radiation. These data are digitally processed and transmitted to ground station.\nTwo main ways of scanning and acquire image data:\n\n\n\nWhiskbroom\nPushbroom\n\n\n\n\nAcross-track scanning\nAlong-track scanning\n\n\nWide swath width\nNarrow swath width\n\n\nSimple optical system\nComplex optical system\n\n\nShorter dwell time\nLonger dwell time\n\n\nPixel distortion\nNo pixel distortion\n\n\nExample: Landsat 7\nExample: Landsat 8\n\n\n\n\n\n\n\n\nWhishbroom. Source: wikipedia\n\n\n\n\n\n\n\n\n\nPushbroom. Source: wikipedia\n\n\n\n\n\n\n3.1.2 Why remote sensing data need to be processed?\nThis may cause by some noise that are produced by sensors, the atmosphere, the terrain, and so on. For example, the atmosphere causes some noise in collecting data, like atmospheric pollution, floating particles. These noise can be removed by using some correction measures.\n\n\n3.1.3 Corrections of image data\n\n3.1.3.1 Geometric correction\nWhen collecting image data, there are some geometric errors. These images can be adjusted by using scale, orientation, and projection, which can match the spatial characteristics of earth surface.\nReasons for image distortions\n\nView angle\nTopography (like hills or not flat ground)\nWind (Using plane to collect data)\nRotation of the earth (Using satellite to collect data)\n\nSolutions for geometric correction\n\nidentify ground control points\nbuild models to provide geometric transformation coefficients\ntransform coordinates of pixels locations\n\n\n\n\n\n\nCoordinates transformation. Source: wikipedia\n\n\n\n\n\nresample pixels values\n\n\n\n\n\n\nResample pixels. Source:Studley 2011\n\n\n\n\nExample for correct a old map in GIS\n\n\n\n\n\nGeometric correction for old map. Source: Yanbo\n\n\n\n\n\n\n3.1.3.2 Atmospheric correction\nEarth’s atmosphere has water vapor and ozone and other gases, and they will have effects on the measuring process. These effects can be removed when using remote sensing data, which is atmospheric correction.\nNecessary and unnecessary atmospheric correction :\n\n\n\n\n\n\n\nUnnecessary\nNecessary\n\n\n\n\nClassification of a single image\nBiophysical parameters needed (e.g. temperature, leaf area index, NDVI)\n\n\nIndependent classification of multi date imagery\nUsing spectral signatures through time and space\n\n\nComposite images (combining images)\n\n\n\nSingle dates or where training data extracted from all data\n\n\n\n\nSource: CASA0023 slides P26\nTypes of atmospheric correction:\nRelative\nnormalize intensities of different bands within a single image and bands from many dates to one date\nAbsolute\nuse atmospheric conditions and illumination at the time of image collection, which to estimate the amount of scattering and absorption in images. Then, change digital brightness values into scaled surface reflectance. This can be done through atmospheric radiative transfer models.\n\n\n\n3.1.3.3 Orthorectification correction\nThe topography of earth has variation, like hills, ridges, canyon and so on. Also, the satellite will have tilt when they collect data. Both of them have effects on the images. Basically, various topographical features and angles of satellite causes more inherent distortion in remote sensing data.\n\n\n\n\n\nSource: Satellite Imaging Corporation\n\n\n\n\nThis is a example of orthorectification, and the left picture shows crooked road, but the right picture shows the real straight road after orthorectification.\n\n\n\n\n\nExample of before orthorectification and after Source: OSSIM\n\n\n\n\n\n\n3.1.3.4 Radiometric Calibration\nAlthough many imagery data has already been converted to radiance by data provider, the process of radiometric calibration should be noticed. It refers to conversion of digital number(DN) to spectral radiance. DN is values in every pixels, and they are observed and measured by sensors. The spectral radiance is the amount of energy per unit area, like the amount of light from sensor in the field of view. There are many factors that affects radiance. For example, the intensity of illumination, the direction of illumination, orientation, position of target feature, and so on.\n\n\n\n\n\nFactors that affect radiance Source: Susan 2020\n\n\n\n\n\n\n\n3.1.4 Mosaicking and Image enhancement\nMosaicking\nIt is similar to merge function in QGIS. For instance, some image data can be merged together based on the characteristics of each image, finally produces a new image.\n\n\n\n\n\nMosaicking Source: ArcGis\n\n\n\n\nImage enhancement\nIn order to better visualize characteristics on images, image can be enhanced by using many different methods, like stretch and spatial filtering. The following picture shows contrast is enhanced, and the visual interpretation can be easier.\n\n\n\n\n\nLinear contrast stretch Source: CCRS"
  },
  {
    "objectID": "week3.html#application",
    "href": "week3.html#application",
    "title": "3  Week3 Remote sensing data",
    "section": "3.2 Application",
    "text": "3.2 Application\nExplorations of radiometric calibration methods in UAS data\nThere are many methods for radiometric calibration. Generally, when you have some remote sensing data, image producers will have some recommended methods, but there are also some empirical methods. One research had compared five common methods by using unmanned aircraft systems data, which tries to explore the best performance of radiometric calibration. The five methods are one-point calibration (method A), one-point calibration plus sunshine sensor (method B), pre-calibration using the simplified empirical line calibration (method C) , one-point calibration plus sunshine sensor plus post-calibration(method D), and post-calibration using the simplified empirical line calibration (method E). The paper found there are no method that can have the best performance in every band Poncet et al. (2019a).\n\n\n\n\n\nRMSE values in all methods Source: Aurelie et al. 2019\n\n\n\n\n\n\n\n\n\nThe distributions of RMSE values in each band Source: Aurelie et al. 2019\n\n\n\n\nThe RMSE values are various in different bands, and they should be always noticed. For example, method B has lower RMSE in red-edge and NIR, but higher in green band. So we need to select the suitable method for radiometric calibration depending on our objectives Poncet et al. (2019b). The analysis of imagery data can be more careful because the radiometric calibration method may have influence on the interpretation of UAS imagery.\nEffects of Atmospheric Corrections on NDVI\nVegetation measurement is an essential application in remote sensing, and NDVI is Normalized Difference Vegetation Index (NDVI), which can quantify vegetation health. For example, when NDVI values are close to +1, the vegetation should be dense green, and when NDVI values are close to 0, there are no green. The NDVI formula is following:\n\n\n\n\n\nNDVI formula: GISGeography\n\n\n\n\n\n\n\n\n\nNDVI values Source: GISGeography\n\n\n\n\nOne study explored common methods (QUAC, FLAASH, DOS, ACOLITE, 6S, and Sen2Cor) for atmospheric Corrections on NDVI by using Landsat 8, Sentinel-2, and UAV sensors data. In real raster data analysis, data probably are from different source, so the atmospheric correction, to some extent, make them more comparable. In this paper, FLAASH method has the best distinctiveness on NDVI between Landsat 8 and Sentinel-2. Moravec et al. (2021).\n\n\n\n\n\nMedian values of non-corrected TOA and corrected BOA Source: David 2021"
  },
  {
    "objectID": "week3.html#reflection",
    "href": "week3.html#reflection",
    "title": "3  Week3 Remote sensing data",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\nImage enhancement\nIn the week 1, the false color composites can enhance the characteristics of image, and I think image enhancement also plays a similar role in visualizing image. In QGIS, there are many options for contrast enhancement, and I can set the min/max value for raster data.\n\n\n\n\n\nContrast enhancement Source: Yanbo\n\n\n\n\n\n\n\n\n\nNo enhancement vs contrast enhancement Source: Yanbo\n\n\n\n\nIn the enhancement image, I can more easily distinguish the boundary of grass and forest, so enhancement is really useful for real analysis in my study.\nMosaicking\nThis is similar to the merge function in QGIS, and merge is essential function. If my study area is not fully recorded by one raster data, I have to mosaic several raster data together in order to get my study area.\nPrincipal Component Analysis\nI think PCA would be a powerful tool in the future study. This contributes to reduce the dimensionality of data. In real remote sensing data analysis, I think the data probably have multiple dimensions, and the PCA would be functional to reduce them, which help us identify which components are more important among the data. For example, I can use PCA in exploring vegetation patterns, which tries to find what vegetation type dominate in study area.\n\n\n\n\nMoravec, David, Jan Komárek, Serafín López-Cuervo Medina, and Iñigo Molina. 2021. “Effect of Atmospheric Corrections on NDVI: Intercomparability of Landsat 8, Sentinel-2, and UAV Sensors.” Remote Sensing 13 (18): 3550. https://doi.org/10.3390/rs13183550.\n\n\nPoncet, Aurelie M., Thorsten Knappenberger, Christian Brodbeck, Michael Fogle, Joey N. Shaw, and Brenda V. Ortiz. 2019a. “Multispectral UAS Data Accuracy for Different Radiometric Calibration Methods.” Remote Sensing 11 (16): 1917. https://doi.org/10.3390/rs11161917.\n\n\n———. 2019b. “Multispectral UAS Data Accuracy for Different Radiometric Calibration Methods.” Remote Sensing 11 (16): 1917. https://doi.org/10.3390/rs11161917."
  },
  {
    "objectID": "week4.html#summary",
    "href": "week4.html#summary",
    "title": "4  Week4 Policy",
    "section": "4.1 Summary",
    "text": "4.1 Summary\nCity: Seattle, US\nSeattle is a vibrant and lively city on the west coast of the US. The city has 4 million people in the metropolitan area. It is one of the fastest-growing large cities, which is home to many renowned companies, including Amazon, Microsoft, Boeing, and Starbucks. Also, the city has diverse cultures, beautiful landscape, great outdoor recreation, and environmental stewardship. Therefore, it has strong cultural and job attractions for the surrounding areas. There are more than four thousand new settlers each year. This requires the city of Seattle to have a better understanding and planning of the city. \nGlobal Policy\nSustainable development goals (SGDs) were formulated in 2015 by the United Nations General Assembly (UNGA), which aims to provide a future development framework. It includes 17 goals that emphasize in the sustainability of social, environment, and economic aspects.\n\n\n\n\n\nSustainable Goals. Source: United Nations\n\n\n\n\n\n\n\n\n\nGoal 11. Source: United Nations\n\n\n\n\n\nTarget 11.7 By 2030, provide universal access to safe, inclusive and accessible, green and public spaces, in particular for women and children, older persons and persons with disabilities.\n\nMetropolitan policy\nThe city of Seattle has a comprehensive plan (2015-2035) in order to become an equitable and sustainable city. The core values of Seattle include, race and social equality, environmental stewardship, community, economic opportunity and security.\n\n\n\n\n\nSeattle 2035. Source: City of Seattle\n\n\n\n\nGoal\n\nENG1: Foster healthy trees, vegetation, and soils to improve human health, provide wildlife habitats, improve drainage, give residents across the city access to nature, provide fresh food, and increase the quality of life for all Seattleites.\n\nPolicies\n\nEN 1.1: Seek to achieve an urban forest that contains a thriving and sustainable mix of tree species and ages, and that creates a contiguous and healthy ecosystem that is valued and cared for by the City and all Seattleites as an essential environmental, economic, and community asset.\n\n\nEN 1.2: Strive to increase citywide tree canopy coverage to 30 percent by 2037 and to 40 percent over time.\n\n\nEN 1.3: Use trees, vegetation, green stormwater infrastructure, amended soil, green roofs, and other low-impact development features to meet drainage needs and reduce the impacts of development. \n\nThese three policies and goals mainly aim to improve the urban forest in Seattle. The urban forest is make up of all individual trees, street trees, green spaces with trees, and associated vegetation in the urban area. Basically, the policies and goals requires city of Seattle have a better management, interpreation, and monitoring of their urban forest.\nSeattle Policy Document"
  },
  {
    "objectID": "week4.html#metropolitan-policy-documents",
    "href": "week4.html#metropolitan-policy-documents",
    "title": "4  Week4 Policy",
    "section": "4.2 Metropolitan policy documents",
    "text": "4.2 Metropolitan policy documents\nThe city of Seattle has a comprehensive plan (2015-2035) in order to become an equitable and sustainable city. The core values of  Seattle include, race and social equality, environmental stewardship, community, economic opportunity and security. However, the goals and policies in the plan don’t provide the detailed solutions that contribute to achieve them. My proposal tries to generate some detailed solutions for environmental goals and policies, which contribute to the health and sustainability of the natural environment in Seattle.\nGoal\nENG1: Foster healthy trees, vegetation, and soils to improve human health, provide wildlife habitats, improve drainage, give residents across the city access to nature, provide fresh food, and increase the quality of life for all Seattleites.\nPolices\nEN 1.1: Seek to achieve an urban forest that contains a thriving and sustainable mix of tree species and ages, and that creates a contiguous and healthy ecosystem that is valued and cared for by the City and all Seattleites as an essential environmental, economic, and community asset. \nEN 1.2: Strive to increase citywide tree canopy coverage to 30 percent by 2037 and to 40 percent over time. \nEN 1.3: Use trees, vegetation, green stormwater infrastructure, amended soil, green roofs, and other low-impact development features to meet drainage needs and reduce the impacts of development. \nSeattle policy link: https://www.seattle.gov/Documents/Departments/OPCD/OngoingInitiatives/SeattlesComprehensivePlan/CouncilAdopted2020.pdf"
  },
  {
    "objectID": "week4.html#application",
    "href": "week4.html#application",
    "title": "4  Week4 Policy",
    "section": "4.2 Application",
    "text": "4.2 Application\nThe goals and policies in the Seattle plan don’t provide the detailed solutions that contribute to achieve them. In order to improve tree canopy cover in Seattle, I think two steps are necessary, and they should be detailed.\n\nexplore the neighborhoods with a high preferablity of planting tree\nidentify the potential planting site in areas with a high priority of tree planting\n\nIn this application, I tries to review some literature and generate some detailed solutions for increasing canopy cover and urban forest, which contribute and support to the health and sustainability of the natural environment in Seattle.\nNeighborhood level: Preferable locations analysis of urban tree planting\nThe study conducted by Locke et al. (2010) analyzed the preferable locations of urban tree planting in New York City, they narrowed the preferable locations to the neighborhood levels. Although the data they used is not remote sensing data, the method of identifying the preferable locations of urban trees are also useful for Seattle. They used need-based criteria for preferability analysis. For example, when some neighborhood with a higher temperature in summer, it would be a high priority area for planting a tree.\n\n\n\n\n\nExamples of variables in preferablity analysis. Source: Locke et al., 2010\n\n\n\n\nThey combined many variables together, and assigned the different weights based on the importance of variables. Finally, they produced a map that shows neighborhoods with different priority for planting trees.\n\n\n\n\n\nPrioritization map of planting trees. Source: Locke et al., 2010\n\n\n\n\nIndividual tree level: the potential planting sites analysis\nThis study conducted by Wu, Xiao, and McPherson (2008) developed a computer program, which virtually planted three types of trees(small, medium, and large trees) in Los Angeles by using satellite data. It can iterativelly find the potential planting areas, and this would be really useful for planting department in every city. The planting department can combine the virtually planting sites with the real site conditions to plant a new tree for city.\ndata\n\nsource：QuickBird satellite\nspatial resolution: 2.4m\nbands: blue, green, red, near-infrared , and a panchromatic band with 60cm\ntemporal resolution: 64 scenes collected from 2002 to 2005\n\nMethod\nThe computer program iterativelly search all the potential planting site, and it will avoid areas that are not suitable to plant trees, like sites too close to the buildings, pavings and so on.\n\n\n\n\n\nFlowchart of tree planting. Source: Wu, Xiao, and McPherson, 2008\n\n\n\n\nOutput\nThe circles are potential planted trees, and the size of circles stands for small, medium, and large trees in the neiborhood.\n\n\n\n\n\nPotential trees planted. Source: Wu, Xiao, and McPherson, 2008"
  },
  {
    "objectID": "week4.html#reflection",
    "href": "week4.html#reflection",
    "title": "4  Week4 Policy",
    "section": "4.3 Reflection",
    "text": "4.3 Reflection\nI think I need to always consider the policies and goals in the application of remote sensing data. The remote sensing data are not only limited in doing research, but also contributed to the development of cities. This will provide a positive loop in both remote sensing development and city development. Also, the policy about increasing green canopy is proposed by many cities, and this would be a great research direction for me. \nThe policies not only have limitations on how to achieve the policies, but also they should provide some detailed guides about tree and vegetation species for citizens. This will engage more citizens to join the planting work, and they may plant more trees in their gardens. It also can avoid some potential risks in the tree planting process. There are many invasive plants in North America, people usually have a high possibility to plant a wrong plant that causes plant invasion.\nThe two cases in the application part only considered the new planting trees. The management and monitoring of existing trees also essential for the achivment of goals and policies. For example, some LiDAR data can be used to detect the height of trees, and also monitoring the tree health. Unhealthy trees can be treated or replaced.\nThe financial part of the implementation of the solution is not considered, and it’s also important for the city of Seattle. The cost of this solution may cause by acquisition of remote sensing data, the implementation of planting trees including planting workers, the costs of nursery trees, and maintenance fees.\n\n\n\n\n\n\nLocke, Dexter H., J. Morgan Grove, Jacqueline W. T. Lu, Austin Troy, Jarlath P. M. O’Niel-Dunne, and Brian D. Beck. 2010. “Prioritizing Preferable Locations for Increasing Urban Tree Canopy in New York City.” Cities and the Environment 3 (1): 1–18. https://doi.org/10.15365/cate.3142010.\n\n\nWu, Chunxia, Qingfu Xiao, and E. Gregory McPherson. 2008. “A Method for Locating Potential Tree-Planting Sites in Urban Areas: A Case Study of Los Angeles, USA.” Urban Forestry & Urban Greening 7 (2): 65–76. https://doi.org/10.1016/j.ufug.2008.01.002."
  },
  {
    "objectID": "week5.html#summary",
    "href": "week5.html#summary",
    "title": "5  Week5 An introduction to Google Earth Engine",
    "section": "5.1 Summary",
    "text": "5.1 Summary\n\n5.1.1 What is Google Earth Engine?\nGoogle Earth Engine is a cloud platform developed by Google, and it can process satellite imagery from Landsat and other geospatial data uploaded by users.\nUser guide\n\n\n5.1.2 Mind map\n\n\n\n\n\nMind map. Source:Yanbo\n\n\n\n\n\n\n5.1.3 Key concepts about GEE\nGEE terms\n\n\n\nGEE\nR\n\n\n\n\nimage\nraster\n\n\nfeature\nvector\n\n\nstack\ncollection\n\n\n\nJavascript\nGEE use javascript (web programming language) to manipulate data.\n\nvar define objects\n// add comments\n\n// key codes in GEE\n\nvar number = 1 \n\nvar string = 'Hello, World!'\n\nvar list = [1.23, 8, -3]\nprint(list[2])\n\nvar dictionary = {\n  a: 'Hello',\n  b: 10,\n  c: 0.1343,\n  d: list\n}\n\nprint(dictionary.b)\nprint(number, string, list, dictionary)\nClient and server\n\n\n\nclient\nserver\n\n\n\n\nweb page on the browser\nall data are stored on the server\n\n\ncode shows on the browser\ncodes runs on the server\n\n\n\nLoop and map\n\ndon’t use loop on the server\nuse mapping functions(only loading image collection once, and get results)\ncode examples\nLoop\nlist = [1,2,3]\n\nbias = 1\n\nx = 0\n\nfor x < list.length:\n  list[x]=list[x]+bias\n\nlist= [2,3,4]\nMap\nlist.map(list=list+1)\n\nlist =[2,3,4]\n\nScale\n\nrefers to pixel resolution\noutput are aggregated (256*256 grid)\nalways need to set a scale, if not, the pixel values may change\n\n\n\n\n\n\nScale. Source: Google Developers\n\n\n\n\nProjection\ndisplays all data into Mercator projection (EPSG: 3857)\n\n\n5.1.4 How to use GEE?\nObjects and method overview\n\n\n\n\n\nCommon Earth Engine object classes. Source: Google Earth Engine\n\n\n\n\nCode editor\n\n\n\n\n\nCode editor. Source: Google Earth Engine\n\n\n\n\nLoad image collection\nuse ee.ImageCollection() to load, filterDate() and filterBounds() to select specific date and locations, otherwise, GEE will have errors because too many elements.\nvar oneimage = ee.ImageCollection('LANDSAT/LC09/C02/T1_L2')\n  .filterDate('2022-01-03', '2022-04-04')\n  .filterBounds(Dheli);  // Intersecting ROI\nMap.addLayer(oneimage, {bands: [\"SR_B4\", \"SR_B3\", \"SR_B2\"]})\nLoad feature collection\nfeatures refers to the geometries with attributes, use ee.FeatureCollection() to load, filter() to select specific area, like R\nvar india = ee.FeatureCollection('users/jesse/china');\nprint(china, \"china\");\nMap.addLayer(china, {}, \"china\");\nvar india = ee.FeatureCollection('users/jesse/china')\n    .filter('GID_1 == \"CHN.25_1\"');\nReducing images\nWhen I only want to classify one image from a massive image collections, use this method. The process of this is to take all median of all images to make a image composite, then summarized into one single image. This process ignored some extreme values, but they may be useful.\ncollection.reduce() client side\nee.Reducer() server side\n// Compute the median in each band, each pixel.\n// Band names are B1_median, B2_median, etc.\nvar median = collection.reduce(ee.Reducer.median());\nby region\nreduceRegion() reduce region a single region\nby region(s)\nreduceRegions() reduce region in many small areas\nby neighborhood\nreduceNeighborhood() reduce by image neighborhood (surrounded pixels)\nLinear regression\nlinearFit() Look change over time. For example, the change of temperature over last decades\n\ndependent variables, like temperature\nindependent varibles, often time\n\nMultiple Linear Regression\nconstant band are required, and need to mentioned the number of X and Y\n.map(createConstantBand)\nvar linearRegression = collection.reduce(\n  ee.Reducer.linearRegression({\n    numX: 2,\n    numY: 2\n}));\nJoin\njoin.apply() similar to joins in R\nExample: add all plants within 100km buffer of the National park\ncode link: https://code.earthengine.google.com/7424ebd78cf7d634f161a0a0619019ea (Source: Andrew Maclachlan)\n\n\n5.1.5 Common examples of GEE Analysis\nmy code link: https://code.earthengine.google.com/a0d4d4aa44552e6a529e39a1d6a3f572\nClip Images\nuse polygon to clip study area\nvar clip = meanImage.clip(india)\n  .select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7']);\n\nvar vis_params3 = {\n  bands: ['SR_B4', 'SR_B3', 'SR_B2'],\n  min: 0,\n  max: 0.3,\n};\n\n// map the layer\nMap.addLayer(clip, vis_params3, 'clip');\n\n\n\n\n\nClip output. Source: Yanbo\n\n\n\n\nTexture measures\nhelp users to extract useful information by applying some mathematical formulas on pixels values. So it can improve the accuracy of image classification and enhance the interpretation of remote sensing data.\n\nuse glcmTexture() in GEE\nsize means the number of neighborhoods, 1 means a 3*3 grid\n\nvar glcm = clip.select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7'])\n  .multiply(1000)\n  .toUint16()\n  .glcmTexture({size: 1})\n  .select('SR_.._contrast|SR_.._diss')\n  .addBands(clip);\n  \n// add to the map, but change the range values  \nMap.addLayer(glcm, {min:14, max: 650}, 'glcm');\n\n\n\n\n\nTexture measures output. Source: Yanbo\n\n\n\n\nPCA\n// scale and band names\n\nvar scale = 30;\nvar bandNames = glcm.bandNames();\nprint('GLCM band names:', glcm.bandNames());\n\nvar region = india.geometry();\nMap.centerObject(region, 10);\nMap.addLayer(ee.Image().paint(region, 0, 2), {}, 'Region');\n\nprint(region, \"india_geometry\");\n\n// mean center the data and SD strech the princapal components \n// and an SD stretch of the principal components.\nprint('bandNames:', bandNames);\nvar meanDict = glcm.reduceRegion({\n    reducer: ee.Reducer.mean(),\n    geometry: region,\n    scale: scale,\n    maxPixels: 1e9\n});\nprint('Mean Dictionary:', meanDict);\nvar means = ee.Image.constant(meanDict.values(bandNames));\nprint(\"Means: \", means);\nvar centered = glcm.subtract(means);\n\n// This helper function returns a list of new band names.\nvar getNewBandNames = function(prefix) {\n  var seq = ee.List.sequence(1, bandNames.length());\n  return seq.map(function(b) {\n    return ee.String(prefix).cat(ee.Number(b).int());\n  });\n};\n\n\n// This function accepts mean centered imagery, a scale and\n// a region in which to perform the analysis.  It returns the\n// Principal Components (PC) in the region as a new image.\nvar getPrincipalComponents = function(centered, scale, region) {\n  // Collapse the bands of the image into a 1D array per pixel.\n  var arrays = centered.toArray();\n\n  // Compute the covariance of the bands within the region.\n  var covar = arrays.reduceRegion({\n    reducer: ee.Reducer.centeredCovariance(),\n    geometry: region,\n    scale: scale,\n    maxPixels: 1e9\n  });\n\n  // Get the 'array' covariance result and cast to an array.\n  // This represents the band-to-band covariance within the region.\n  var covarArray = ee.Array(covar.get('array'));\n\n  // Perform an eigen analysis and slice apart the values and vectors.\n  var eigens = covarArray.eigen();\n\n  // This is a P-length vector of Eigenvalues.\n  var eigenValues = eigens.slice(1, 0, 1);\n  // This is a PxP matrix with eigenvectors in rows.\n  \n  var eigenValuesList = eigenValues.toList().flatten();\n  var total = eigenValuesList.reduce(ee.Reducer.sum());\n  var percentageVariance = eigenValuesList.map(function(item) {\n  return (ee.Number(item).divide(total)).multiply(100).format('%.2f')});\n  \n  print(\"percentageVariance\", percentageVariance);\n\n  var eigenVectors = eigens.slice(1, 1);\n\n  // Convert the array image to 2D arrays for matrix computations.\n  var arrayImage = arrays.toArray(1);\n\n  // Left multiply the image array by the matrix of eigenvectors.\n  var principalComponents = ee.Image(eigenVectors).matrixMultiply(arrayImage);\n\n  // Turn the square roots of the Eigenvalues into a P-band image.\n  var sdImage = ee.Image(eigenValues.sqrt())\n    .arrayProject([0]).arrayFlatten([getNewBandNames('sd')]);\n\n  // Turn the PCs into a P-band image, normalized by SD.\n  return principalComponents\n    // Throw out an an unneeded dimension, [[]] -> [].\n    .arrayProject([0])\n    // Make the one band array image a multi-band image, [] -> image.\n    .arrayFlatten([getNewBandNames('pc')])\n    // Normalize the PCs by their SDs.\n    .divide(sdImage);\n};\n\n// Get the PCs at the specified scale and in the specified region\nvar pcImage = getPrincipalComponents(centered, scale, region);\n\n\nMap.addLayer(pcImage, {bands: ['pc2', 'pc1'], min: -2, max: 2}, 'PCA bands 1 and 2');\np1 and p2 has the highest percentage, and they are the most important components. Here, extract pc1 and pc2.\n\n\n\n\n\nPCA percentage list. Source: Yanbo\n\n\n\n\n\n\n\n\n\nPCA output. Source: Yanbo\n\n\n\n\nNDVI\nimportant index that helps vegetation analysis in many cases.\n// common methods\nvar NDVI_1 = clip.select('SR_B5').subtract(clip.select('SR_B4'))\n  .divide(clip.select('SR_5').add(clip.select('SR_B4')));\n// GEE functions\nvar NDVI_2 = clip.normalizedDifference([SR_B5, SR_B4]);\nMap.addLayer(NDVI_1, { min: -1, max: 1, palette: ['blue', 'white', 'green']}, 'NDVI');\n\n\n\n\n\nNDVI output. Source: Yanbo\n\n\n\n\nExport layers\nuse Export.image.toDrive()\n// Export the image, specifying the CRS, transform, and region.\nExport.image.toDrive({\n  image: PCA_out,\n  description: 'PCA_india',\n  scale:30,\n  crs: projection.crs,\n  maxPixels: 100E10,\n  region: bounds\n\n});"
  },
  {
    "objectID": "week5.html#application",
    "href": "week5.html#application",
    "title": "5  Week5 An introduction to Google Earth Engine",
    "section": "5.2 Application",
    "text": "5.2 Application"
  },
  {
    "objectID": "week5.html#reflection",
    "href": "week5.html#reflection",
    "title": "5  Week5 An introduction to Google Earth Engine",
    "section": "5.3 Reflection",
    "text": "5.3 Reflection"
  },
  {
    "objectID": "week6.html#summary",
    "href": "week6.html#summary",
    "title": "6  Week6 Classification I",
    "section": "6.1 Summary",
    "text": "6.1 Summary"
  },
  {
    "objectID": "week6.html#application",
    "href": "week6.html#application",
    "title": "6  Week6 Classification I",
    "section": "6.2 Application",
    "text": "6.2 Application"
  },
  {
    "objectID": "week6.html#reflection",
    "href": "week6.html#reflection",
    "title": "6  Week6 Classification I",
    "section": "6.3 Reflection",
    "text": "6.3 Reflection"
  },
  {
    "objectID": "week7.html#summary",
    "href": "week7.html#summary",
    "title": "7  Week7 Classification II",
    "section": "7.1 Summary",
    "text": "7.1 Summary"
  },
  {
    "objectID": "week7.html#application",
    "href": "week7.html#application",
    "title": "7  Week7 Classification II",
    "section": "7.2 Application",
    "text": "7.2 Application"
  },
  {
    "objectID": "week7.html#reflection",
    "href": "week7.html#reflection",
    "title": "7  Week7 Classification II",
    "section": "7.3 Reflection",
    "text": "7.3 Reflection"
  },
  {
    "objectID": "week8.html#summary",
    "href": "week8.html#summary",
    "title": "8  Week8 Temperature",
    "section": "8.1 Summary",
    "text": "8.1 Summary"
  },
  {
    "objectID": "week8.html#application",
    "href": "week8.html#application",
    "title": "8  Week8 Temperature",
    "section": "8.2 Application",
    "text": "8.2 Application"
  },
  {
    "objectID": "week8.html#reflection",
    "href": "week8.html#reflection",
    "title": "8  Week8 Temperature",
    "section": "8.3 Reflection",
    "text": "8.3 Reflection"
  },
  {
    "objectID": "week9.html#summary",
    "href": "week9.html#summary",
    "title": "9  Week9 SAR",
    "section": "9.1 Summary",
    "text": "9.1 Summary"
  },
  {
    "objectID": "week9.html#application",
    "href": "week9.html#application",
    "title": "9  Week9 SAR",
    "section": "9.2 Application",
    "text": "9.2 Application"
  },
  {
    "objectID": "week9.html#reflection",
    "href": "week9.html#reflection",
    "title": "9  Week9 SAR",
    "section": "9.3 Reflection",
    "text": "9.3 Reflection"
  }
]