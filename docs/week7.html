<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.313">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Learning Diary for CASA0023 - 7&nbsp; Week7 Classification II</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./week8.html" rel="next">
<link href="./week6.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    <img src="./ucl.jpg" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Learning Diary for CASA0023</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/yyjj2215/learning_diary_CASA0023"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Week7 Classification II</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Welcome</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Week1 Getting started with remote sensing</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Week2 Portfolio</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week3.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Week3 Remote sensing data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week4.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Week4 Policy</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week5.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Week5 An introduction to Google Earth Engine</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week6.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Week6 Classification I</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week7.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Week7 Classification II</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week8.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Week8 Temperature and policy</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Summary</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#summary" id="toc-summary" class="nav-link active" data-scroll-target="#summary"><span class="toc-section-number">7.1</span>  Summary</a>
  <ul class="collapse">
  <li><a href="#mind-map" id="toc-mind-map" class="nav-link" data-scroll-target="#mind-map"><span class="toc-section-number">7.1.1</span>  Mind map</a></li>
  <li><a href="#classification-approaches" id="toc-classification-approaches" class="nav-link" data-scroll-target="#classification-approaches"><span class="toc-section-number">7.1.2</span>  Classification Approaches</a></li>
  <li><a href="#accuracy-assessment" id="toc-accuracy-assessment" class="nav-link" data-scroll-target="#accuracy-assessment"><span class="toc-section-number">7.1.3</span>  Accuracy assessment</a></li>
  <li><a href="#test-and-train-data" id="toc-test-and-train-data" class="nav-link" data-scroll-target="#test-and-train-data"><span class="toc-section-number">7.1.4</span>  Test and train data</a></li>
  <li><a href="#applications-of-classification-approaches-on-gee" id="toc-applications-of-classification-approaches-on-gee" class="nav-link" data-scroll-target="#applications-of-classification-approaches-on-gee"><span class="toc-section-number">7.1.5</span>  Applications of classification approaches on GEE</a></li>
  <li><a href="#useful-pre-classified-data-resource" id="toc-useful-pre-classified-data-resource" class="nav-link" data-scroll-target="#useful-pre-classified-data-resource"><span class="toc-section-number">7.1.6</span>  Useful pre-classified data resource</a></li>
  </ul></li>
  <li><a href="#application" id="toc-application" class="nav-link" data-scroll-target="#application"><span class="toc-section-number">7.2</span>  Application</a>
  <ul class="collapse">
  <li><a href="#case1-pixel--vs.-object-based-classification-using-landsat-data-on-gee" id="toc-case1-pixel--vs.-object-based-classification-using-landsat-data-on-gee" class="nav-link" data-scroll-target="#case1-pixel--vs.-object-based-classification-using-landsat-data-on-gee"><span class="toc-section-number">7.2.1</span>  Case1: Pixel- vs.&nbsp;Object-based classification using Landsat data on GEE</a></li>
  <li><a href="#case2-trends-in-remote-sensing-accuracy-assessment-approaches-in-the-context-of-natural-resources" id="toc-case2-trends-in-remote-sensing-accuracy-assessment-approaches-in-the-context-of-natural-resources" class="nav-link" data-scroll-target="#case2-trends-in-remote-sensing-accuracy-assessment-approaches-in-the-context-of-natural-resources"><span class="toc-section-number">7.2.2</span>  Case2: <strong>Trends in Remote Sensing Accuracy Assessment Approaches in the Context of Natural Resources</strong></a></li>
  </ul></li>
  <li><a href="#reflection" id="toc-reflection" class="nav-link" data-scroll-target="#reflection"><span class="toc-section-number">7.3</span>  Reflection</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/yyjj2215/learning_diary_CASA0023/edit/main/week7.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/yyjj2215/learning_diary_CASA0023/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Week7 Classification II</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="summary" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="summary"><span class="header-section-number">7.1</span> Summary</h2>
<section id="mind-map" class="level3" data-number="7.1.1">
<h3 data-number="7.1.1" class="anchored" data-anchor-id="mind-map"><span class="header-section-number">7.1.1</span> Mind map</h3>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="week7_data/mind.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Mind map. Source: Yanbo</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="classification-approaches" class="level3" data-number="7.1.2">
<h3 data-number="7.1.2" class="anchored" data-anchor-id="classification-approaches"><span class="header-section-number">7.1.2</span> Classification Approaches</h3>
<section id="object-based-image-analysis" class="level4" data-number="7.1.2.1">
<h4 data-number="7.1.2.1" class="anchored" data-anchor-id="object-based-image-analysis"><span class="header-section-number">7.1.2.1</span> Object based image analysis</h4>
<p>In classification, a pixel can’t represent an object, and an object normally is made up of many pixels. There are many methods in object based image analysis. Superpixels consider the similarity of pixels and homogeneity of the pixels, and Simple Linear Iterative Clustering (SLIC) is one of most common method to generate superpixels. For example, the centroid of objects moves iteratively, and the images become more object-based images.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="week7_data/ob.gif" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Supercells. Source: <a href="https://jakubnowosad.com/ogh2021/#10">Nowosad 2021</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Bears in right picture are objected based, and similar pixels are clustered as superpixels.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="week7_data/bear.png" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Supercells. Source: <a href="https://darshita1405.medium.com/superpixels-and-slic-6b2d8a6e4f08">Darshite Jain</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
<p><em>Other algorithms</em></p>
<ul>
<li>multi-resolution segmentation in <a href="https://geospatial.trimble.com/products-and-solutions/trimble-ecognition">eCognition Definiens Developer</a></li>
</ul>
</section>
<section id="sub-pixel-analysis" class="level4" data-number="7.1.2.2">
<h4 data-number="7.1.2.2" class="anchored" data-anchor-id="sub-pixel-analysis"><span class="header-section-number">7.1.2.2</span> Sub pixel analysis</h4>
<p>It was also called Spectral Mixture Analysis, Linear spectral unmixing. It proivdes the details in every single pixel, and it estimates the fractions that made up of this pixel. For example, it shows how many percent of urban area, and how many percent of vegetation, and how many percent of water.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="week7_data/sub.png" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Source: <a href="https://www.researchgate.net/publication/259715697_IDENTIFYING_MULTI-DECADAL_CHANGES_OF_THE_SAO_PAULO_URBAN_AGGLOMERATION_WITH_MIXED_REMOTE_SENSING_TECHNIQUES_SPECTRAL_MIXTURE_ANALYSIS_AND_NIGHT_LIGHTS/figures?lo=1">Machado and Small 2013</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>In terms of how it works, the linear sum of endmembers weighted by associated endmember fraction represents the reflectance in each bands. For example, water is 13 in Band 3, and this is a endmember. Then, the percentage of different objects can be calculated based on endmembers.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="week7_data/band.png" class="img-fluid figure-img" style="width:40.0%"></p>
<p></p><figcaption class="figure-caption">Source: <a href="https://andrewmaclachlan.github.io/CASA0023-lecture-7/?panelset1=data2#18">Andrew 2023</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="accuracy-assessment" class="level3" data-number="7.1.3">
<h3 data-number="7.1.3" class="anchored" data-anchor-id="accuracy-assessment"><span class="header-section-number">7.1.3</span> Accuracy assessment</h3>
<p>The evaluation is essential part in the classification, and different evaluation methods may give us different performances of our classification. The terms in remote sensing are different in machine learning, and we need to balance the accuracy between producer accuracy and user accuracy because it’ hard to get the best in both of them.</p>
<p><strong>Producer, user and overall accuracy</strong></p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;">Remote Sensing</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Machine Learning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Producer accuracy (PA)</td>
<td style="text-align: center;">TP/TP+FN</td>
<td style="text-align: center;">Recall</td>
</tr>
<tr class="even">
<td style="text-align: center;">User accuracy (UA)</td>
<td style="text-align: center;">TP/TP+FP</td>
<td style="text-align: center;">Precision</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Overall accuracy (OA)</td>
<td style="text-align: center;">TP+TN/TP+FP+FN+TN</td>
<td style="text-align: center;">Overall accuracy</td>
</tr>
</tbody>
</table>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="week7_data/matrix.png" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption">Confusion Matrix Source: <a href="https://www.v7labs.com/blog/confusion-matrix-guide">Rohit 2023</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
<p><strong>Errors</strong></p>
<p>provide performance of our classification. The following matrix shows error matrix in remote sensing.</p>
<ul>
<li><p>Error of omission = 100 - PA</p>
<p>Open land = 42/180 = 24%</p>
<p>Open land producer = 138/180= 76%</p></li>
<li><p>Error of commission = 100 - UA</p>
<p>Open land = 68/206 = 33%</p>
<p>Open land user = 138/206= 67%</p></li>
</ul>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="week7_data/error.png" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Error Matrix Source: <a href="https://www.chegg.com/homework-help/questions-and-answers/4-based-error-matrix-accuracy-report-calculate-following-15-points-error-matrix-example-su-q47737786">Chegg</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
<p><strong>Kappa</strong></p>
<p>express the accuracy of an image compared to the results by chance. However, it has two limitations, one is hard to define a good value, and the other is Kappa values have different levels of accuracy.</p>
<p><strong>F1 score</strong></p>
<p>combines producer accuracy and user accuracy, range from 0 to 1, and 1 is the best performance. It also has two limitations, it doesn’t consider the true negative, and it considers user accuracy and producer are equally important.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="week7_data/F1.png" class="img-fluid figure-img" style="width:80.0%"></p>
</figure>
</div>
</div>
</div>
<p><strong>Receiver Operating Characteristic Curve (ROC Curve)</strong></p>
<p>is a graph showing the performance of classifier at all classification thresholds.</p>
<p>AUC (area under the ROC Curve) stands for all area under the entire ROC Curve.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="week7_data/ROC.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">ROC and AUC. Source: <a href="https://mlu-explain.github.io/roc-auc/">Jared 2022</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="test-and-train-data" class="level3" data-number="7.1.4">
<h3 data-number="7.1.4" class="anchored" data-anchor-id="test-and-train-data"><span class="header-section-number">7.1.4</span> Test and train data</h3>
<p><strong>Cross-validation</strong></p>
<p>a resampling method that uses different portions of the data to test and train a classifier on different iterations. The accuracy is the average of all classifers.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="week7_data/cv.png" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption class="figure-caption">Cross validation. Source: <a href="https://scikit-learn.org/stable/modules/cross_validation.html">scikit learn</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
<p><strong>Spatial cross validation</strong></p>
<p>is the cross validation that consider spatial dependence. Spatial autocorrelation are happens between training and testing data. Spatial cross validation spatially partitioning the folded data, and stops training and testing data being near each other. For example, if there are two seprate forest in the image, and the training data are from the first forest, and the testing data are from second forest.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="week7_data/scv.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Cross validation vs Spatial cross validation. Source: <a href="https://towardsdatascience.com/spatial-cross-validation-using-scikit-learn-74cb8ffe0ab9">Chiara 2021</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="applications-of-classification-approaches-on-gee" class="level3" data-number="7.1.5">
<h3 data-number="7.1.5" class="anchored" data-anchor-id="applications-of-classification-approaches-on-gee"><span class="header-section-number">7.1.5</span> Applications of classification approaches on GEE</h3>
<p>The approaches including subpixel, superpixel, and object-based have been introduced in the previous part, and here I used GEE to do a classification for Tianjian, China on GEE. The key steps were summarized in the following workflow.</p>
<p>My code link is <a href="https://code.earthengine.google.com/4eaa3862a6f9c091e16da1f2c8b82ff2" class="uri">https://code.earthengine.google.com/4eaa3862a6f9c091e16da1f2c8b82ff2</a></p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="week7_data/workflow.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Workflow. Source: Yanbo</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The following pictures are my results. In the super pixel, the pixels were clustered as a bigger pixel. The differences among small pixels were ignored, and they become a bigger pixel. In object-based, if there are more higher resolution data, the results will be more better than current one.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="week7_data/prac.png" class="img-fluid figure-img" style="width:100.0%"></p>
<p></p><figcaption class="figure-caption">Results of practical. Source: Yanbo</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>In subpixel, the classified results are based on the percent of each pixel. If the pixel is greater than 0.5, then give this pixel a value, like 1 refers to high_urban.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="week7_data/sub_percent.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Percent of each endmember per pixel. Source: Yanbo</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="useful-pre-classified-data-resource" class="level3" data-number="7.1.6">
<h3 data-number="7.1.6" class="anchored" data-anchor-id="useful-pre-classified-data-resource"><span class="header-section-number">7.1.6</span> Useful pre-classified data resource</h3>
<p>Many pre-classified data we can use in our future analysis, and the Dynamic World also combine deep learning approaches in their classification.</p>
<ol type="1">
<li><p>GlobeLand30 - 30m for 2000, 2010 and 2020: <a href="http://www.globallandcover.com/home_en.html?type=data" class="uri">http://www.globallandcover.com/home_en.html?type=data</a></p></li>
<li><p>European Space Agency’s (ESA) Climate Change Initiative (CCI) annual global land cover (300 m) (1992-2015): <a href="https://climate.esa.int/en/projects/land-cover/data/" class="uri">https://climate.esa.int/en/projects/land-cover/data/</a></p></li>
<li><p>Dynamic World - near real time 10m: <a href="https://www.dynamicworld.app/explore/" class="uri">https://www.dynamicworld.app/explore/</a></p></li>
<li><p>MODIS: <a href="https://modis.gsfc.nasa.gov/data/dataprod/mod12.php" class="uri">https://modis.gsfc.nasa.gov/data/dataprod/mod12.php</a></p></li>
<li><p>Google building data: <a href="https://sites.research.google/open-buildings/" class="uri">https://sites.research.google/open-buildings/</a></p></li>
</ol>
<p>Source: <a href="https://andrewmaclachlan.github.io/CASA0023-lecture-7/?panelset1=data2#6">Andrew 2023</a></p>
</section>
</section>
<section id="application" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="application"><span class="header-section-number">7.2</span> Application</h2>
<p>There are many classification approaches in land use and land cover(LULC) classification. Object-based and subpixel were mentioned in course content. Also, many accuracy assessment metrics were introduced. This application tries to explore how classification approaches were applied in research, and how to justify the accuracy assessment metrics in our future analysis.</p>
<section id="case1-pixel--vs.-object-based-classification-using-landsat-data-on-gee" class="level3" data-number="7.2.1">
<h3 data-number="7.2.1" class="anchored" data-anchor-id="case1-pixel--vs.-object-based-classification-using-landsat-data-on-gee"><span class="header-section-number">7.2.1</span> Case1: Pixel- vs.&nbsp;Object-based classification using Landsat data on GEE</h3>
<p>Like we explored in last week application, the resolutions of data and classification algorithms (RF, CART, SVM, etc.) have effects on the performance of classification. The selection of classification approaches is also important, and this paper provides some explorations about object-based approach. <span class="citation" data-cites="tassi2021">Tassi et al. (<a href="references.html#ref-tassi2021" role="doc-biblioref">2021a</a>)</span> conducted this case study in Maiella National Park, Italy, and compared four different approaches based on Random Forest.</p>
<table class="table">
<tbody>
<tr class="odd">
<td style="text-align: center;">PB</td>
<td>Pixel-based</td>
</tr>
<tr class="even">
<td style="text-align: center;">PBT</td>
<td>Pixel-based including the image textural information</td>
</tr>
<tr class="odd">
<td style="text-align: center;">OB</td>
<td>Object-Based, using BDC</td>
</tr>
<tr class="even">
<td style="text-align: center;">OBP</td>
<td>Object-Based, using the L8 15-m panchromatic band and the BDC</td>
</tr>
</tbody>
</table>
<p>PBT approach has texture features than PB approach, and texture features are extracted using GLCM that we explored in week5. OBP approach has the 15-m L8 pan band, which was generated by the L8 30 meter RGB bands using ‘rgbToHsv’ and ‘hsvToRgb’. Both OB approach and OBP approach applied SNIC to cluster pixels in GEE. The evaluation metrics were including OA, PA, UA, and F-score. <span class="citation" data-cites="tassi2021a">Tassi et al. (<a href="references.html#ref-tassi2021a" role="doc-biblioref">2021b</a>)</span> found OB approach has no accuracy improvement than PB approach, but the OBP approach has significantly improved the accuracy than OB approach. Therefore, when we use OB approach in the classification, we can consider add a new 15-meter panchromatic band in order to improve the accuracy. In addition, I think this paper doesn’t have enough discussion about why adding a new band will improve accuracy. In my opinion, the better classification of object-based approach still relies on the better resolution bands because the new 15 meters band has higher spatial resolution than the 30 meters bands in Landsat 8.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="week7_data/a1.png" class="img-fluid figure-img" style="width:75.0%"></p>
<p></p><figcaption class="figure-caption">Framework of analysis. Source: <a href="https://www.mdpi.com/2072-4292/13/12/2299#">Tassi et al.&nbsp;2021</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The following figures also clearly shows the accuracy of OBP is the biggest among all accuracy assessment metrics, and the error of OBP is the minimum among all approaches.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="week7_data/a3.png" class="img-fluid figure-img" style="width:75.0%"></p>
<p></p><figcaption class="figure-caption">OA, UA, PA, F-score. Source: <a href="https://www.mdpi.com/2072-4292/13/12/2299#">Tassi et al.&nbsp;2021</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="week7_data/a2.png" class="img-fluid figure-img" style="width:75.0%"></p>
<p></p><figcaption class="figure-caption">Out of bag errors and number of trees. Source: <a href="https://www.mdpi.com/2072-4292/13/12/2299#">Tassi et al.&nbsp;2021</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="case2-trends-in-remote-sensing-accuracy-assessment-approaches-in-the-context-of-natural-resources" class="level3" data-number="7.2.2">
<h3 data-number="7.2.2" class="anchored" data-anchor-id="case2-trends-in-remote-sensing-accuracy-assessment-approaches-in-the-context-of-natural-resources"><span class="header-section-number">7.2.2</span> Case2: <strong>Trends in Remote Sensing Accuracy Assessment Approaches in the Context of Natural Resources</strong></h3>
<p>Several evaluation metrics were illustrated in the previous summary, but applications of these metrics are limited. Therefore, this part tries to provide more information about the selection of accuracy assessment approaches. <span class="citation" data-cites="morales-barquero2019">Morales-Barquero et al. (<a href="references.html#ref-morales-barquero2019" role="doc-biblioref">2019a</a>)</span> examined all the papers (282) including accuracy assessment terms in the Web of Science from 1998 to 2017, which aimed to provide a guideline to all accuracy assessment approaches for remote sensing researches. The following figures show there are no standardized accuracy assessment, and most common accuracy assessment approaches are OA and UA. Although Kappa has some limitations, it is still reported in many papers. Overall, the error matrix is the most common and useful in classification, and I think it will be one of great metric in the classification. Also, I think the selection of accuracy assessment approaches are various depending on the research purposes.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="week7_data/a4.png" class="img-fluid figure-img" style="width:75.0%"></p>
<p></p><figcaption class="figure-caption">Type of accuracy assessment metrics in different scales category. Source: <a href="https://www.mdpi.com/2072-4292/13/12/2299#">Morales-Barquero et al.&nbsp;2019</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="week7_data/a6.png" class="img-fluid figure-img" style="width:75.0%"></p>
<p></p><figcaption class="figure-caption">Accuracy assessment metrics summary. Source: <a href="https://www.mdpi.com/2072-4292/13/12/2299#">Morales-Barquero et al.&nbsp;2019</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The another interesting finding in their paper is the relationship between OA and validation unit. When the number of units for polygon (group of pixels) increases, the OA also increases (<span class="citation" data-cites="morales-barquero2019a">Morales-Barquero et al. (<a href="references.html#ref-morales-barquero2019a" role="doc-biblioref">2019b</a>)</span>). So when we consider to select validation data, we can use polygon.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="week7_data/a5.png" class="img-fluid figure-img" style="width:75.0%"></p>
<p></p><figcaption class="figure-caption">Relation between overall accuracy and validation unit. Source: <a href="https://www.mdpi.com/2072-4292/13/12/2299#">Morales-Barquero et al.&nbsp;2019</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="reflection" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="reflection"><span class="header-section-number">7.3</span> Reflection</h2>
<p>Cross-validation is a common and important technique that can help me to evaluate my model, avoid overfitting, and optimize hyperparameters in my model training work. Similarly, spatial cross-validation has not only previous benefits but also considers spatial autocorrelation. In this method, it makes the training and testing datasets are spatially independent. This will improve the accuracy of the spatial prediection model. Therefore, I think spatial cross validation will be a useful tool in evaluation of my future spatial prediction model.</p>
<p>This week’s summary covered many accuracy assessment methods, but I think the most useful one is confusion matrix in the classification. In the matrix, I can directly see the performance of my model in each category. For example, I can find that my model probably classified some vegetation as forest because they are similar or my model classified all water correctly. This will provide some ideas about where my model did well or not. I can adjust my model based on the confusion matrix. In addition, I can consider to improve my UA, PA or OA depending on various purposes. For example, if I do some analysis about the glacier, I probably don’t care about the accuracy of vegetation in my model.</p>
<p>Like the first case study in the application, it compares the performance of pixed-based and object-based approaches, and they found object-based approach has higher accuracy. In my future work, I think I also need to explore different approaches, like object-based and subpixel. This will give me more information about how to justify them, and select the suitable approach. The second case shows the accuracy assessment methods are varied in many papers. Although some of the studys still use Kappa, I think Kappa is not the best accuracy assessment compared to OA, UA, and PA in the classification problems. In my opinion, confusion matrix is the first evaluation method when I do classification in machine learning.</p>
<p>In the practical, the subpixel analysis requires endmembers to compute the percent of each pixel, and I used mean of selected ROI as the values of endmembers. In this process, I only select one big polygon for each category, and this may reduce the accuracy of my classification. Also, In my classification, I divide urban into two categories: high-urban and low-urban, I found that low-urban is so diverse, and it’s hard to find the best sample of low urban on the map. I think I need to explore more about how to better classify low-urban and high-urban. One of my idea about this probably is that I can combine some GIS data with helping me distinguish them. In addition, the superpixel uses SNIC algorithms based on color and spatial proximity, which is very interesting to me. I found this algorithm also can be applied to scene analysis, which may be useful for my dissertation topic. My dissertation is to analyze the urban green space using deep learning based on street view imagery.</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-morales-barquero2019" class="csl-entry" role="doc-biblioentry">
Morales-Barquero, Lucia, Mitchell B. Lyons, Stuart R. Phinn, and Chris M. Roelfsema. 2019a. <span>“Trends in Remote Sensing Accuracy Assessment Approaches in the Context of Natural Resources.”</span> <em>Remote Sensing</em> 11 (19): 2305. <a href="https://doi.org/10.3390/rs11192305">https://doi.org/10.3390/rs11192305</a>.
</div>
<div id="ref-morales-barquero2019a" class="csl-entry" role="doc-biblioentry">
———. 2019b. <span>“Trends in Remote Sensing Accuracy Assessment Approaches in the Context of Natural Resources.”</span> <em>Remote Sensing</em> 11 (19): 2305. <a href="https://doi.org/10.3390/rs11192305">https://doi.org/10.3390/rs11192305</a>.
</div>
<div id="ref-tassi2021" class="csl-entry" role="doc-biblioentry">
Tassi, Andrea, Daniela Gigante, Giuseppe Modica, Luciano Di Martino, and Marco Vizzari. 2021a. <span>“Pixel- Vs. Object-Based Landsat 8 Data Classification in Google Earth Engine Using Random Forest: The Case Study of Maiella National Park.”</span> <em>Remote Sensing</em> 13 (12): 2299. <a href="https://doi.org/10.3390/rs13122299">https://doi.org/10.3390/rs13122299</a>.
</div>
<div id="ref-tassi2021a" class="csl-entry" role="doc-biblioentry">
———. 2021b. <span>“Pixel- Vs. Object-Based Landsat 8 Data Classification in Google Earth Engine Using Random Forest: The Case Study of Maiella National Park.”</span> <em>Remote Sensing</em> 13 (12): 2299. <a href="https://doi.org/10.3390/rs13122299">https://doi.org/10.3390/rs13122299</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./week6.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Week6 Classification I</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./week8.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Week8 Temperature and policy</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>