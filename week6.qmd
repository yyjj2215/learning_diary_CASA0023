# Week6 Classification I

## Summary

### Mind map

```{r echo=FALSE, out.width = "100%", fig.align='left', cache=FALSE, fig.cap= "Mind map. Source: Yanbo"}
knitr::include_graphics('week6_data/mindmap.png') 
```

### **Applications of classified data**

*Monitoring forests ----- illegal logging*

The illegal logging case is very interesting, because illegal loggers use technical ways to avoid the monitoring of forests. When researchers classify the imagery data of forests, small logging areas are ignored because the resolution of imagery (250\*250m) is larger than the size of logging areas. These illegal logging areas are hidden in the picture and not able to be detected. Therefore, the high resolution data is important in the classification problems.

```{r echo=FALSE, out.width = "100%", fig.align='left', cache=FALSE, fig.cap= "Illegal logging patch. Source: [Image courtesy of DEMA-AP](https://news.mongabay.com/2019/04/how-a-sheriff-in-brazil-is-using-satellites-to-stop-deforestation/)"}
knitr::include_graphics('week6_data/illegal.png') 
```

### Machine learning methods

#### **Decision tree (CART)**

It can apply in both classification and regression problem, which is a binary tree. CART uses the Gini coefficient to make splits, and it represents the model's impurity. The smaller the Gini coefficient indicates the lower the impurity, and the better outputs.

```{r echo=FALSE, out.width = "80%", fig.align='left', cache=FALSE, fig.cap= "Gini Impurity formula. Source: [Huy Bui](https://towardsdatascience.com/decision-tree-fundamentals-388f57a60d2a)"}
knitr::include_graphics('week6_data/gini.png') 
```

***Classification***

The predicted values are classified into two or more, and they are **discrete**.

The example of decision tree:

Should I take CASA0023?

```{r echo=FALSE, out.width = "80%", fig.align='left', cache=FALSE, fig.cap= "Decision tree. Source: Yanbo"}
knitr::include_graphics('week6_data/DT.png') 
```

***Regression***

The predicted values are **continuous**, like housing price, GCSE scores.

```{r echo=FALSE, out.width = "100%", fig.align='left', cache=FALSE, fig.cap= "Decision tree regression. Source: [scikit-learn](https://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/auto_examples/tree/plot_tree_regression.html)"}
knitr::include_graphics('week6_data/DTR.png') 
```

**Overfitting**

When the max depth of tree is high (red line in previous figure), the tree learn too fine details, so it casues overfitting problems.

*Two ways to avoid overfitting:*

-   limit how trees grow (eg. a minimum number of pixels in a leaf, 20 is often used)

-   Weakest link pruning (with tree score)

    Tree score = SSR (The sum of squared residuals)+ tree penalty (alpha) \* T (number of leaves)

*How to do in code?*

-   limit number of leaves in each tree

-   change Alpha

*Hyperparameter-tuning*

It is a process that finds the best parameter (Alpha in Decision Tree) of model in machine learning.

```{r echo=FALSE, out.width = "100%", fig.align='left', cache=FALSE, fig.cap= "Hyperparameter tuning Worlflow. Source: Yanbo"}
knitr::include_graphics('week6_data/flow.png') 
```

#### **Random forest**

It is made up of many decision trees. It reduces the risk of overfitting because it make decision tree from random number of variables (never all of them), and take a random subset of variables again.

```{r echo=FALSE, out.width = "100%", fig.align='left', cache=FALSE, fig.cap= "Decision tree vs Random forest. Source: [Rosaria Silipo](https://towardsdatascience.com/from-a-single-decision-tree-to-a-random-forest-b9523be65147)"}
knitr::include_graphics('week6_data/rf.png') 
```

Bootstrapping is re-sampling and replacing data to make a decision.

```{r echo=FALSE, out.width = "100%", fig.align='left', cache=FALSE, fig.cap= "Random forest classifier on a dataset. Source: [Siddharth Misra, Hao Li](https://www.sciencedirect.com/science/article/pii/B9780128177365000090)"}
knitr::include_graphics('week6_data/rf.jpg') 
```

#### Support Vector Machine (SVM)

This algorithm is to find a hyperplane in a N-dimensional space that can classify all data. It introduces in the framework of structural risk minimization (SRM).

```{r echo=FALSE, out.width = "100%", fig.align='left', cache=FALSE, fig.cap= "SVM. Source: [javaTpoint](https://www.javatpoint.com/machine-learning-support-vector-machine-algorithm)"}
knitr::include_graphics('week6_data/svm1.png') 
```

SVM in 3D uses a plane instead of a line when there are more than two datasets.

```{r echo=FALSE, out.width = "100%", fig.align='left', cache=FALSE, fig.cap= "Hyperplanes in 2D and 3D. Source: [Rohith Gandhi](https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47)"}
knitr::include_graphics('week6_data/svm2.png') 
```

***Hyperparameter-tuning in SVM***

-   Type of kernel (rbf, poly, linear, sigmoid)

-   C controls training data and decision boundary maximisation plus margin errors

```{r echo=FALSE, out.width = "100%", fig.align='left', cache=FALSE, fig.cap= "C = 1000, 50, 5. Source: [Dishaa Agarwal](https://www.analyticsvidhya.com/blog/2021/04/insight-into-svm-support-vector-machine-along-with-code/)"}
knitr::include_graphics('week6_data/svm3.png') 
```

-   Gamma (or Sigma) control radius for classified points

```{r echo=FALSE, out.width = "100%", fig.align='left', cache=FALSE, fig.cap= "Gamma = 0, 10, 100. Source: [Sunil Ray](https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/)"}
knitr::include_graphics('week6_data/svm4.png') 
```

### How image classification works?

Image classification turn every pixel in the image into one pre-defined categorical classification. The following picture shows how computer see in image . In remote sensing, if the pixel values are similar, and they probably are the same classes. The previous machine learning methods are used to classify these pixel values.

```{r echo=FALSE, out.width = "100%", fig.align='left', cache=FALSE, fig.cap= "Source: [Thilo Huellmann](https://levity.ai/blog/how-do-image-classifiers-work)"}
knitr::include_graphics('week6_data/dog.jpeg') 
```

```{r echo=FALSE, out.width = "100%", fig.align='left', cache=FALSE, fig.cap= "Supervised vs Unsupervised procedures. Source: Yanbo"}
knitr::include_graphics('week6_data/procedure.png') 
```

### Classification on GEE

My case in Hong Kong code link: <https://code.earthengine.google.com/0158a598d6dd7961dcfeacd9854ebff7>

```{r echo=FALSE, out.width = "100%", fig.align='left', cache=FALSE, fig.cap= "Classification workflow in GEE. Source: Yanbo"}
knitr::include_graphics('week6_data/classification_workflow.png') 
```

The left picture is the output from CART and it doesn't split training and testing data. The right picture is the output from Random Forest, and the classifier was trained with 70% training pixels, and the training accuracy is 0.9980750721847931, the validation accuracy is 0.9956616052060737. The left output classify more forest areas than the right output.

In my case, class 1 is water, class 2 is forest, class 3 is urban.

```{r echo=FALSE, out.width = "100%", fig.align='left', cache=FALSE, fig.cap= "CART output vs RF output. Source: Yanbo"}
knitr::include_graphics('week6_data/final_output.png') 
```

## Application

## Reflection
